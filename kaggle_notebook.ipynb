{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch, gc, os\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import recall_score, precision_score\n",
    "\n",
    "class NERTrainer():\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "                 train_dataloader, \n",
    "                 valid_dataloader, \n",
    "                 optimizer, \n",
    "                 scheduler, \n",
    "                 num_epochs,\n",
    "                 id2label, \n",
    "                 output_dir,\n",
    "                 patience,\n",
    "                 fold):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.num_epochs = num_epochs\n",
    "        self.output_dir = output_dir\n",
    "        self.id2label = id2label\n",
    "        self.patience = patience\n",
    "        self.fold = fold\n",
    "\n",
    "    def clear_memories(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    def compute_metrics(self,logits, labels):\n",
    "        predictions = logits.argmax(-1).numpy()\n",
    "        true_labels = [[ self.id2label[l] for l in label if l != -100] for label in labels.numpy()]\n",
    "        true_predictions = [[self.id2label[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels.numpy())]\n",
    "        \n",
    "        score = f1_score(y_true = true_labels, y_pred=true_predictions, average='macro')\n",
    "        return score\n",
    "    \n",
    "    def compute_f5_score(self, logits, labels):\n",
    "        predictions = logits.argmax(-1).numpy()\n",
    "        true_labels = [[ self.id2label[l] for l in label if l != -100] for label in labels.numpy()]\n",
    "        true_predictions = [[self.id2label[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels.numpy())]\n",
    "        recall = recall_score(true_labels, true_predictions)\n",
    "        precision = precision_score(true_labels, true_predictions)\n",
    "        f5_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n",
    "        return f5_score\n",
    "    \n",
    "    def train_one_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        \n",
    "        running_loss = 0\n",
    "        progress_bar = tqdm(enumerate(self.train_dataloader),total=len(self.train_dataloader))\n",
    "        scores = []\n",
    "        \n",
    "        for step, data in progress_bar:\n",
    "            out = self.model(**data)\n",
    "            loss = out.loss\n",
    "            logits = out.logits\n",
    "            \n",
    "            loss.backward()\n",
    "            # print(optimizer)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            scores.append(self.compute_f5_score(logits=logits.detach().cpu(),\n",
    "                                               labels= data['labels'].detach().cpu()))\n",
    "            \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "                \n",
    "            running_loss += loss.item()\n",
    "            epoch_loss = running_loss/(step+1)\n",
    "            \n",
    "            score = sum(scores)/len(scores)\n",
    "            progress_bar.set_postfix(Epoch = epoch,\n",
    "                                    TrainingLoss = epoch_loss,\n",
    "                                    F1 = score\n",
    "                                    )\n",
    "            \n",
    "        del out, loss, logits\n",
    "        self.clear_memories()\n",
    "        return epoch_loss, score\n",
    "    \n",
    "    def valid_one_epoch(self, epoch):\n",
    "        self.model.eval()\n",
    "        \n",
    "        running_loss = 0\n",
    "        progress_bar = tqdm(enumerate(self.valid_dataloader),total=len(self.valid_dataloader))\n",
    "        scores = []\n",
    "        for index, data in progress_bar:\n",
    "            with torch.no_grad():\n",
    "                out = self.model(**data)\n",
    "            loss = out.loss\n",
    "            logits = out.logits\n",
    "                \n",
    "            running_loss += loss.item()\n",
    "            epoch_loss = running_loss/(index+1)\n",
    "            scores.append(self.compute_f5_score(logits=logits.detach().cpu(),\n",
    "                            labels= data['labels'].detach().cpu()))\n",
    "            \n",
    "            score = sum(scores)/len(scores)\n",
    "            progress_bar.set_postfix(Epoch = epoch,\n",
    "                                    ValidationLoss = epoch_loss,\n",
    "                                    F1 = score\n",
    "                                    )\n",
    "            \n",
    "        del out, loss, logits\n",
    "        self.clear_memories()\n",
    "        return epoch_loss, score\n",
    "\n",
    "    def __call__(self):\n",
    "        print('\\n')\n",
    "        prev_best_loss = np.inf\n",
    "        best_score = -np.inf\n",
    "        model_output_dir=self.output_dir\n",
    "        \n",
    "        early_break_count = 0\n",
    "        for epoch in range(self.num_epochs):\n",
    "            training_loss, training_score = self.train_one_epoch(epoch = epoch)\n",
    "            \n",
    "            validation_loss, validation_score = self.valid_one_epoch(epoch = epoch)\n",
    "            \n",
    "            print('='*170 + '\\n')\n",
    "            print(f'Fold- {self.fold}, epoch- {epoch}')\n",
    "            print(f'Training Loss for epoch: {epoch} is {training_loss}, F1 Score is: {training_score}')\n",
    "            print(f'Validation Loss for epoch: {epoch} is {validation_loss}, F1 Score is: {validation_score}')\n",
    "\n",
    "            if validation_score > best_score:\n",
    "                print(f'F1 Score improved from {best_score} --> {validation_score}')\n",
    "                best_score = validation_score\n",
    "                \n",
    "                checkpoint_dir = os.path.join(model_output_dir,f'Checkpoint-Fold-{self.fold}-F1')\n",
    "                \n",
    "                if not os.path.exists(model_output_dir):\n",
    "                    os.mkdir(model_output_dir)\n",
    "                if not os.path.exists(checkpoint_dir):\n",
    "                    os.mkdir(os.path.join(checkpoint_dir))\n",
    "                    \n",
    "                self.model.save_pretrained(save_directory = checkpoint_dir)\n",
    "                print(f\"Model Saved at {checkpoint_dir}\")\n",
    "                \n",
    "                if validation_score > 0.95:\n",
    "                    break\n",
    "                \n",
    "            elif validation_loss < prev_best_loss:\n",
    "                print(f'Loss improved from {prev_best_loss} --> {validation_loss}')\n",
    "                prev_best_loss = validation_loss\n",
    "                \n",
    "                checkpoint_dir = os.path.join(model_output_dir,f'Checkpoints-Fold-{self.fold}-loss')\n",
    "                \n",
    "                if not os.path.exists(model_output_dir):\n",
    "                    os.mkdir(model_output_dir)\n",
    "                if not os.path.exists(checkpoint_dir):\n",
    "                    os.mkdir(os.path.join(checkpoint_dir))\n",
    "                \n",
    "                self.model.save_pretrained(save_directory = checkpoint_dir)\n",
    "                print(f\"Model Saved at {checkpoint_dir}\")\n",
    "                \n",
    "            else:\n",
    "                early_break_count +=1\n",
    "                print(f'Early break is at {early_break_count}, will stop training at {self.patience}')\n",
    "                if early_break_count >= self.patience:\n",
    "                    print(f'Early Stopping')\n",
    "                    break\n",
    "                            \n",
    "            print('\\n' + '='*170)\n",
    "        \n",
    "        print(f'Training over with best loss: {prev_best_loss} and best F1: {best_score}')\n",
    "        \n",
    "        fold_dir = os.path.join(model_output_dir, f'Fold-{self.fold}')\n",
    "        if not os.path.exists(os.path.join(model_output_dir, f'Fold-{self.fold}')):\n",
    "            os.mkdir(fold_dir)\n",
    "        self.model.save_pretrained(save_directory = fold_dir)\n",
    "        \n",
    "        print(f'Model saved at {model_output_dir}')\n",
    "        print('='*170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, torch\n",
    "from torch.utils.data import Dataset\n",
    "from banglanlptoolkit import BnNLPNormalizer\n",
    "\n",
    "def create_id_label_conversion(unique_labels):\n",
    "    id2label = {}\n",
    "    label2id = {}\n",
    "    for index, label in enumerate((unique_labels)):\n",
    "        label2id[label] = int(index)\n",
    "        id2label[index] = label\n",
    "    return id2label, label2id, len(id2label)\n",
    "\n",
    "class preprocess():\n",
    "    def __init__(self, \n",
    "                 label2id = None,\n",
    "                 return_target_ids = True,\n",
    "                 stopword_dict=[], \n",
    "                 stopword_remove = True,\n",
    "                 punct_remove = True, \n",
    "                 to_lower = True, \n",
    "                 strip = True):\n",
    "        \n",
    "        self.to_lower = to_lower\n",
    "        self.return_ids = return_target_ids\n",
    "        self.punct_remove = punct_remove\n",
    "        self.strip = strip\n",
    "        self.stopword_dict = stopword_dict\n",
    "        self.stopword_remove = stopword_remove\n",
    "        self.label2id = label2id\n",
    "        \n",
    "    def remove_punctuations(self, text, label):\n",
    "        if self.punct_remove:\n",
    "            text = re.sub(r'[^\\w\\s]', '', text.strip())\n",
    "            text = re.sub(r'[|iœОабвгдезиклмнопрстуцчщь。いくけしたてなを一不业中丰为了产人们任优会使便保內其力务務区口可各后吸售回國地場孔家富専己市引心懂成手技抗护挽捷措撬改教文断新施晦普晰服术来果業標正洋流海涩清源漁焕然物現畅留発的目真瞳研碑磨社科章続练细育自艺节行表見见論质资距进遗重鑽門间难]','',text.strip())\n",
    "            text = re.sub(r'[Éàáãçéêíя–—，’]', '', text.strip())\n",
    "            return re.sub(r'[_]', '', text.strip()) # if label == 'O' else text\n",
    "        else:\n",
    "            return text\n",
    "    \n",
    "    def remove_stopwords(self, text, label):\n",
    "        if not self.stopword_remove:\n",
    "            return text\n",
    "        if text in self.stopword_dict:\n",
    "            return '' if label == 'O' else text\n",
    "        else:\n",
    "            return text\n",
    "    \n",
    "    def strip_empty_strings(self, tokens, labels):\n",
    "        new_labels = []\n",
    "        new_tokens = []\n",
    "        if not self.strip:\n",
    "            return tokens, labels\n",
    "        \n",
    "        for token, label in zip(tokens, labels):\n",
    "            if token != '':\n",
    "                new_tokens.append(token)\n",
    "                if self.return_ids and self.label2id != None:\n",
    "                    new_labels.append(self.label2id[label])\n",
    "                else:\n",
    "                    new_labels.append(label)\n",
    "        return new_tokens, new_labels\n",
    "        \n",
    "    def __call__(self, tokens, labels):\n",
    "        if self.to_lower:\n",
    "            tokens = [self.remove_punctuations(text, label).lower() for text, label in zip(tokens, labels)]\n",
    "        else:\n",
    "            tokens = [self.remove_punctuations(text, label) for text, label in zip(tokens, labels)]\n",
    "        \n",
    "        tokens = [self.remove_stopwords(text, label) for text, label in zip(tokens, labels)]\n",
    "        tokens, labels = self.strip_empty_strings(tokens, labels)\n",
    "        if len(tokens) != len(labels):\n",
    "            raise ValueError(f'The length of tokens are {len(tokens)} while the length of labels are {len(labels)}')\n",
    "        return tokens, labels\n",
    "    \n",
    "class tokenize_data():\n",
    "    def __init__(self, \n",
    "                 tokenizer, \n",
    "                 add_special_tokens=False, \n",
    "                 max_length=None, \n",
    "                 padding=False, \n",
    "                 truncation=None):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.padding = padding\n",
    "        self.truncation = truncation\n",
    "        self.add_special_tokens = add_special_tokens\n",
    "        \n",
    "    def align_labels_to_ids(self, labels, word_id_list):\n",
    "        new_labels = []\n",
    "        prev_word_id = -1\n",
    "        none_label_token = -101\n",
    "        \n",
    "        for _, word_id in enumerate(word_id_list):\n",
    "            if word_id is None:\n",
    "                new_labels.append(none_label_token)\n",
    "            elif word_id != prev_word_id:\n",
    "                prev_word_id = word_id\n",
    "                new_labels.append(labels[word_id])\n",
    "            elif word_id == prev_word_id:\n",
    "                prev_word_id = word_id\n",
    "                new_labels.append(labels[word_id])\n",
    "        return new_labels\n",
    "    \n",
    "    def __call__(self, tokens, labels):\n",
    "        tokenized_inputs = self.tokenizer(' '.join(tokens), \n",
    "                                          add_special_tokens=self.add_special_tokens, \n",
    "                                          truncation=self.truncation, \n",
    "                                          padding=self.padding, \n",
    "                                          max_length = self.max_length)\n",
    "        word_id_list = tokenized_inputs.word_ids()\n",
    "        labels = self.align_labels_to_ids(labels, word_id_list)\n",
    "        if len(tokenized_inputs['input_ids']) != len(labels):\n",
    "            raise ValueError(f\"The length of tokenized inputs are {len(tokenized_inputs['input_ids'])} while the length of labels are {len(labels)}\")\n",
    "        return tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'], labels\n",
    "    \n",
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer, device):\n",
    "        self.tokenizer= tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        output= {}\n",
    "        max_len = max([len(ids['input_ids']) for ids in batch])\n",
    "        \n",
    "        output[\"input_ids\"] = [sample['input_ids'] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        output['labels'] = [sample['labels'] for sample in batch]\n",
    "        \n",
    "        max_len= max([len(ids) for ids in output['input_ids']])\n",
    "        \n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"input_ids\"] = torch.tensor([ids + (max_len - len(ids))*[self.tokenizer.pad_token_id] for ids in output['input_ids']], dtype=torch.long, device=self.device)\n",
    "            output['attention_mask']= torch.tensor([mask + (max_len - len(mask))*[0] for mask in output['attention_mask']], dtype=torch.long, device=self.device)\n",
    "            output['labels']= torch.tensor([target + (max_len - len(target))*[-100] for target in output['labels']], dtype=torch.long, device=self.device)\n",
    "        else:\n",
    "            output[\"input_ids\"] = torch.tensor([(max_len - len(ids))*[self.tokenizer.pad_token_id] + ids for ids in output['input_ids']], dtype=torch.long, device=self.device)\n",
    "            output['attention_mask']= torch.tensor([(max_len - len(mask))*[0] + mask for mask in output['attention_mask']], dtype=torch.long, device=self.device)\n",
    "            output['labels']= torch.tensor([(max_len - len(target))*[-100] + target for target in output['labels']], dtype=torch.long, device=self.device)\n",
    "\n",
    "        return output\n",
    "    \n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data,\n",
    "                 label2id,\n",
    "                 max_length,\n",
    "                 tokenizer,\n",
    "                 stopwords\n",
    "                 ):\n",
    "        \n",
    "        self.label2id = label2id\n",
    "        self.tokens = data.tokens\n",
    "        self.labels = data.labels\n",
    "        self.max_length = max_length\n",
    "        self.preprocess_fn = preprocess(label2id=self.label2id,stopword_dict=stopwords)\n",
    "        self.tokenizer_fn = tokenize_data(tokenizer = tokenizer, max_length=self.max_length, truncation=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tokens, labels = self.preprocess_fn(self.tokens[index], self.labels[index])\n",
    "        input_ids, attention_mask, labels = self.tokenizer_fn(tokens, labels)\n",
    "        return {'input_ids' : input_ids, 'attention_mask' : attention_mask, 'labels' : labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json, torch, gc, os, glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, AutoModelForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from data_utils import create_id_label_conversion, CustomDataCollator, NERDataset\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from train_utils import NERTrainer\n",
    "\n",
    "class CONFIG:\n",
    "    train_debug = False\n",
    "    seeds = [42] #[0, 42, 43, 50] # Random seeds for each fold\n",
    "    dataset_path = ['DATA/processed_train.json','External Data']\n",
    "    model_path = \"microsoft/deberta-v3-base\" #'dslim/bert-base-NER'\n",
    "    stopword_dir = 'json_folder/stopwords.json'\n",
    "    train_batch_size = 2\n",
    "    valid_batch_size = 2\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    learning_rate = 2e-5\n",
    "    weight_decay = 0.1\n",
    "    output_dir = 'Test' if train_debug else 'Models/Train_Deberta_V3'\n",
    "    num_epochs = 3 if train_debug else 100\n",
    "    T_max = 500\n",
    "    min_lr = learning_rate\n",
    "    max_length = 128 if train_debug else 1024\n",
    "    patience = 9\n",
    "    \n",
    "# Load NLTK Stopwords for English\n",
    "with open(CONFIG.stopword_dir,'r') as f:\n",
    "    stopwords = json.load(f)['english']\n",
    "\n",
    "# Load Data from CSV\n",
    "data = pd.DataFrame()\n",
    "for data_path in CONFIG.dataset_path:\n",
    "    if os.path.isfile(data_path):\n",
    "        data = pd.concat([data, pd.DataFrame({'tokens':pd.read_json(data_path)['tokens'].tolist(), 'labels': pd.read_json(data_path)['labels'].tolist()})])\n",
    "    elif os.path.isdir(data_path):\n",
    "        files = glob.glob(data_path + '/*')\n",
    "        for file in files:\n",
    "            data = pd.concat([data, pd.DataFrame({'tokens':pd.read_json(file)['tokens'].tolist(), 'labels': pd.read_json(file)['labels'].tolist()})])\n",
    "\n",
    "# Calculate id2label and label2id using unique labels from dataframe\n",
    "unique_labels = pd.DataFrame.explode(data,column='labels').labels.unique()\n",
    "id2label, label2id, num_labels = create_id_label_conversion(unique_labels)\n",
    "\n",
    "for fold, seed in enumerate(CONFIG.seeds):\n",
    "    print('\\n'+'+'*170)\n",
    "    print(f'Training Starting for Fold {fold}')\n",
    "    # Split and Fold\n",
    "    train_data, valid_data = train_test_split(data, test_size=0.2, random_state=seed)\n",
    "    train_data.reset_index(drop=True,inplace=True)\n",
    "    valid_data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    # Define and initialize necessary modules\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG.model_path)    \n",
    "    data_collator_fn = CustomDataCollator(tokenizer=tokenizer, device=CONFIG.device)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(pretrained_model_name_or_path=CONFIG.model_path,\n",
    "                                    id2label = id2label,\n",
    "                                    label2id = label2id,\n",
    "                                    num_labels = num_labels,\n",
    "                                    ignore_mismatched_sizes = True\n",
    "                                    ).to(CONFIG.device)\n",
    "\n",
    "    # Initialize Dataset and DataLoader\n",
    "    train_dataset = NERDataset(train_data, label2id=label2id, max_length=CONFIG.max_length, tokenizer=tokenizer, stopwords=stopwords)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=CONFIG.train_batch_size, collate_fn=data_collator_fn, shuffle=True, pin_memory=False)\n",
    "\n",
    "    valid_dataset = NERDataset(valid_data, label2id=label2id, max_length=CONFIG.max_length, tokenizer=tokenizer, stopwords=stopwords)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=CONFIG.valid_batch_size, collate_fn=data_collator_fn, shuffle=False, pin_memory=False)\n",
    "\n",
    "    print('\\n'+'='*170)\n",
    "    print(f'Training will start with {len(train_dataset)} training datapoints and {len(valid_dataset)} validation datapoints.')\n",
    "    print(f'Batch size being used for training is {CONFIG.train_batch_size} and maximum length for datapoints are {CONFIG.max_length}')\n",
    "    print(f'The unique label number is {num_labels}, unique labels are {id2label.values()}')\n",
    "    print('='*170)\n",
    "\n",
    "    # Initiate Optimizer and Scheduler\n",
    "    optimizer= AdamW(model.parameters(), lr= CONFIG.learning_rate, weight_decay= CONFIG.weight_decay)\n",
    "    scheduler= lr_scheduler.CosineAnnealingLR(optimizer, T_max= CONFIG.T_max, eta_min= CONFIG.min_lr)\n",
    "\n",
    "    # Training Loop\n",
    "    NERTrainer(model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            valid_dataloader=valid_dataloader,\n",
    "            num_epochs=CONFIG.num_epochs,\n",
    "            id2label = id2label,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            output_dir=CONFIG.output_dir,\n",
    "            patience = CONFIG.patience,\n",
    "            fold=fold)()\n",
    "    \n",
    "    del model, optimizer, scheduler, train_dataset, valid_dataset, train_dataloader, valid_dataloader\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(f'Fold {fold}, finished training.')\n",
    "    print('+'*170)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
